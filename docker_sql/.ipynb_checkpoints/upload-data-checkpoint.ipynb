{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61fe5694-3934-47f5-acc2-f3cbc2d9ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05677ce5-1874-4808-a525-9f47f4febb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b234383-9a1d-41bb-aab4-bc7e1f43d95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad6afa39-ab45-40e5-8fcf-66fe04a1bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./week1/yellow_tripdata_2021-01.csv.gz',nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "213d7a58-799a-46c7-a6b7-9e37b46685f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f7253cb-2505-43ce-a881-ada5b6491663",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3c257d6-9016-43d2-b998-fedcd2fc502c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x7fa018ed8b20>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b0f35b9-4401-4f6c-bffc-314965f5881c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE yellow_taxi_data (\n",
      "\t\"VendorID\" BIGINT, \n",
      "\ttpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\ttpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tpassenger_count BIGINT, \n",
      "\ttrip_distance FLOAT(53), \n",
      "\t\"RatecodeID\" BIGINT, \n",
      "\tstore_and_fwd_flag TEXT, \n",
      "\t\"PULocationID\" BIGINT, \n",
      "\t\"DOLocationID\" BIGINT, \n",
      "\tpayment_type BIGINT, \n",
      "\tfare_amount FLOAT(53), \n",
      "\textra FLOAT(53), \n",
      "\tmta_tax FLOAT(53), \n",
      "\ttip_amount FLOAT(53), \n",
      "\ttolls_amount FLOAT(53), \n",
      "\timprovement_surcharge FLOAT(53), \n",
      "\ttotal_amount FLOAT(53), \n",
      "\tcongestion_surcharge FLOAT(53)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pd.io.sql.get_schema(df, name='yellow_taxi_data', con=engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e12b2841-37b1-4c83-8cd4-f7f8bc156313",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iter = pd.read_csv('./week1/yellow_tripdata_2021-01.csv.gz', iterator=True, chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8f05795-fcb8-409a-a9d6-1f1e10338a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = next(df_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e62184a-f935-419e-9ed9-2450e007cd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d03d173f-2444-4ef2-af8a-e50b513109a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c2c245-45cf-4ad7-99f3-4d0472041508",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df827c2a-17dd-4b5c-9b6e-6964ce4c9a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=0).to_sql(name='yellow_taxi_data', con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7235f2d4-3b2e-4be3-ae58-cd6a3baad00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 632 ms, sys: 21.2 ms, total: 653 ms\n",
      "Wall time: 958 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df.to_sql(name='yellow_taxi_data', con=engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77f2feea-13b1-4f4f-ba56-63b364954ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted another chunk...took 1.079 second\n",
      "inserted another chunk...took 1.092 second\n",
      "inserted another chunk...took 0.867 second\n",
      "inserted another chunk...took 0.975 second\n",
      "inserted another chunk...took 0.985 second\n",
      "inserted another chunk...took 0.917 second\n",
      "inserted another chunk...took 1.070 second\n",
      "inserted another chunk...took 1.012 second\n",
      "inserted another chunk...took 0.909 second\n",
      "inserted another chunk...took 1.008 second\n",
      "inserted another chunk...took 0.883 second\n",
      "inserted another chunk...took 1.047 second\n",
      "inserted another chunk...took 1.128 second\n",
      "inserted another chunk...took 0.890 second\n",
      "inserted another chunk...took 1.005 second\n",
      "inserted another chunk...took 1.005 second\n",
      "inserted another chunk...took 0.970 second\n",
      "inserted another chunk...took 0.928 second\n",
      "inserted another chunk...took 0.912 second\n",
      "inserted another chunk...took 1.048 second\n",
      "inserted another chunk...took 1.001 second\n",
      "inserted another chunk...took 0.863 second\n",
      "inserted another chunk...took 1.242 second\n",
      "inserted another chunk...took 0.941 second\n",
      "inserted another chunk...took 0.949 second\n",
      "inserted another chunk...took 1.020 second\n",
      "inserted another chunk...took 0.929 second\n",
      "inserted another chunk...took 1.002 second\n",
      "inserted another chunk...took 0.944 second\n",
      "inserted another chunk...took 0.884 second\n",
      "inserted another chunk...took 1.008 second\n",
      "inserted another chunk...took 1.107 second\n",
      "inserted another chunk...took 1.138 second\n",
      "inserted another chunk...took 0.897 second\n",
      "inserted another chunk...took 0.889 second\n",
      "inserted another chunk...took 1.003 second\n",
      "inserted another chunk...took 0.944 second\n",
      "inserted another chunk...took 1.024 second\n",
      "inserted another chunk...took 0.905 second\n",
      "inserted another chunk...took 0.911 second\n",
      "inserted another chunk...took 0.960 second\n",
      "inserted another chunk...took 1.058 second\n",
      "inserted another chunk...took 0.920 second\n",
      "inserted another chunk...took 1.160 second\n",
      "inserted another chunk...took 0.958 second\n",
      "inserted another chunk...took 0.912 second\n",
      "inserted another chunk...took 0.981 second\n",
      "inserted another chunk...took 0.900 second\n",
      "inserted another chunk...took 1.099 second\n",
      "inserted another chunk...took 0.989 second\n",
      "inserted another chunk...took 0.916 second\n",
      "inserted another chunk...took 0.989 second\n",
      "inserted another chunk...took 0.946 second\n",
      "inserted another chunk...took 1.144 second\n",
      "inserted another chunk...took 0.921 second\n",
      "inserted another chunk...took 1.014 second\n",
      "inserted another chunk...took 0.987 second\n",
      "inserted another chunk...took 0.962 second\n",
      "inserted another chunk...took 1.024 second\n",
      "inserted another chunk...took 0.915 second\n",
      "inserted another chunk...took 0.936 second\n",
      "inserted another chunk...took 1.295 second\n",
      "inserted another chunk...took 0.999 second\n",
      "inserted another chunk...took 1.056 second\n",
      "inserted another chunk...took 1.026 second\n",
      "inserted another chunk...took 0.979 second\n",
      "inserted another chunk...took 0.995 second\n",
      "inserted another chunk...took 1.069 second\n",
      "inserted another chunk...took 1.014 second\n",
      "inserted another chunk...took 0.888 second\n",
      "inserted another chunk...took 0.982 second\n",
      "inserted another chunk...took 0.896 second\n",
      "inserted another chunk...took 1.008 second\n",
      "inserted another chunk...took 1.090 second\n",
      "inserted another chunk...took 1.196 second\n",
      "inserted another chunk...took 0.950 second\n",
      "inserted another chunk...took 0.899 second\n",
      "inserted another chunk...took 1.088 second\n",
      "inserted another chunk...took 0.923 second\n",
      "inserted another chunk...took 1.085 second\n",
      "inserted another chunk...took 0.967 second\n",
      "inserted another chunk...took 0.933 second\n",
      "inserted another chunk...took 1.026 second\n",
      "inserted another chunk...took 1.279 second\n",
      "inserted another chunk...took 1.120 second\n",
      "inserted another chunk...took 1.049 second\n",
      "inserted another chunk...took 0.901 second\n",
      "inserted another chunk...took 1.105 second\n",
      "inserted another chunk...took 1.005 second\n",
      "inserted another chunk...took 1.008 second\n",
      "inserted another chunk...took 0.922 second\n",
      "inserted another chunk...took 1.080 second\n",
      "inserted another chunk...took 1.031 second\n",
      "inserted another chunk...took 0.997 second\n",
      "inserted another chunk...took 1.087 second\n",
      "inserted another chunk...took 0.943 second\n",
      "inserted another chunk...took 0.889 second\n",
      "inserted another chunk...took 1.187 second\n",
      "inserted another chunk...took 0.985 second\n",
      "inserted another chunk...took 1.005 second\n",
      "inserted another chunk...took 0.889 second\n",
      "inserted another chunk...took 1.044 second\n",
      "inserted another chunk...took 0.963 second\n",
      "inserted another chunk...took 1.001 second\n",
      "inserted another chunk...took 1.146 second\n",
      "inserted another chunk...took 1.012 second\n",
      "inserted another chunk...took 0.918 second\n",
      "inserted another chunk...took 1.046 second\n",
      "inserted another chunk...took 0.989 second\n",
      "inserted another chunk...took 0.991 second\n",
      "inserted another chunk...took 1.096 second\n",
      "inserted another chunk...took 0.909 second\n",
      "inserted another chunk...took 0.981 second\n",
      "inserted another chunk...took 1.043 second\n",
      "inserted another chunk...took 1.088 second\n",
      "inserted another chunk...took 1.031 second\n",
      "inserted another chunk...took 1.000 second\n",
      "inserted another chunk...took 0.960 second\n",
      "inserted another chunk...took 0.962 second\n",
      "inserted another chunk...took 1.030 second\n",
      "inserted another chunk...took 0.964 second\n",
      "inserted another chunk...took 0.889 second\n",
      "inserted another chunk...took 1.070 second\n",
      "inserted another chunk...took 0.891 second\n",
      "inserted another chunk...took 0.994 second\n",
      "inserted another chunk...took 1.005 second\n",
      "inserted another chunk...took 0.797 second\n",
      "inserted another chunk...took 0.851 second\n",
      "inserted another chunk...took 0.892 second\n",
      "inserted another chunk...took 1.003 second\n",
      "inserted another chunk...took 0.862 second\n",
      "inserted another chunk...took 0.915 second\n",
      "inserted another chunk...took 0.832 second\n",
      "inserted another chunk...took 0.788 second\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     t_start \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m----> 3\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     df\u001b[38;5;241m.\u001b[39mtpep_pickup_datetime \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df\u001b[38;5;241m.\u001b[39mtpep_pickup_datetime)\n\u001b[1;32m      6\u001b[0m     df\u001b[38;5;241m.\u001b[39mtpep_dropoff_datetime \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df\u001b[38;5;241m.\u001b[39mtpep_dropoff_datetime)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1668\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   1667\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1668\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1669\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1670\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1777\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1776\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[0;32m-> 1777\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:868\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    t_start = time()\n",
    "    df = next(df_iter)\n",
    "\n",
    "    df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "    df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "\n",
    "    df.to_sql(name='yellow_taxi_data', con=engine, if_exists='append')\n",
    "\n",
    "    t_end = time()\n",
    "\n",
    "    print('inserted another chunk...took %.3f second' % (t_end - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5bb35e8-1a38-416f-aec1-9296e02e7ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-19 19:25:10--  https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.171.8, 54.231.139.184, 54.231.197.80, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.171.8|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘taxi+_zone_lookup.csv’\n",
      "\n",
      "taxi+_zone_lookup.c 100%[===================>]  12.03K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-01-19 19:25:10 (47.9 MB/s) - ‘taxi+_zone_lookup.csv’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3951d5ca-424a-4c3c-a7de-18988df645e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = pd.read_csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f59ef6-f71a-4ec0-81bf-c6ea298d392d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationID</th>\n",
       "      <th>Borough</th>\n",
       "      <th>Zone</th>\n",
       "      <th>service_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>EWR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Allerton/Pelham Gardens</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>Yellow Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>Arden Heights</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LocationID        Borough                     Zone service_zone\n",
       "0           1            EWR           Newark Airport          EWR\n",
       "1           2         Queens              Jamaica Bay    Boro Zone\n",
       "2           3          Bronx  Allerton/Pelham Gardens    Boro Zone\n",
       "3           4      Manhattan            Alphabet City  Yellow Zone\n",
       "4           5  Staten Island            Arden Heights    Boro Zone"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b55c30bc-72cb-4d0e-984e-da91cb4a1c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones.to_sql(name='zones', con=engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a71d288a-335e-4bda-ba8c-2cd428897b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-21 23:44:11--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-09.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/b5af7693-2f26-4bd5-8854-75edeb650bae?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240121%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240121T234411Z&X-Amz-Expires=300&X-Amz-Signature=554b8a472c3445101bb32b78cdbb8267409c9c77c4e7307d4770b99e36073ef2&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dgreen_tripdata_2019-09.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-01-21 23:44:11--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/b5af7693-2f26-4bd5-8854-75edeb650bae?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240121%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240121T234411Z&X-Amz-Expires=300&X-Amz-Signature=554b8a472c3445101bb32b78cdbb8267409c9c77c4e7307d4770b99e36073ef2&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dgreen_tripdata_2019-09.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7854533 (7.5M) [application/octet-stream]\n",
      "Saving to: ‘green_tripdata_2019-09.csv.gz’\n",
      "\n",
      "green_tripdata_2019 100%[===================>]   7.49M  --.-KB/s    in 0.06s   \n",
      "\n",
      "2024-01-21 23:44:11 (118 MB/s) - ‘green_tripdata_2019-09.csv.gz’ saved [7854533/7854533]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-09.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e176f29e-048a-429b-83ad-b6c884bca4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11029/2392118503.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_green_taxi = pd.read_csv('./green_tripdata_2019-09.csv.gz')\n"
     ]
    }
   ],
   "source": [
    "df_green_taxi = pd.read_csv('green_tripdata_2019-09.csv.gz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
